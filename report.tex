

\documentclass[conference,compsoc]{IEEEtran}


% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi




% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}

% for superscripts like first, second...
\usepackage[super]{nth}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Big Data Processing Systems\\\nth{2} Project}


% author names and affiliations
% use a multiple column layout for up to three different
% affiliations
\author{\IEEEauthorblockN{Helder Rodrigues}
\IEEEauthorblockA{MAEBD\\FCT UNL PT\\Email: harr@campus.fct.unl.pt}

}




% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
This document presents a set of experiments with the aim of showing the capabilities of 2 Big Data technologies, Hive and Spark in 2 flavours DataFrames and SQL, for extracting insights by processing a great volume of data.
\end{abstract}

% no keywords




% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle




\section{Introduction}
% no \IEEEPARstart
We will go trough the Project 2 statement\cite{IEEEhowto:bdpsp1} analysing the  information about taxi rides in some city for better understanding the behavior of the community and help taxi drivers maximize their profit. We will use Big Data technology during our experiments, Hive and Spark in 2 flavours Data Frames and SQL, and take conclusions about the analysis methods. For Spark we used the python API denominated PySpark.

\hfill hr

\hfill December 2, 2020

% \subsection{Subsection Heading Here}
% Subsection text here.


% \subsubsection{Subsubsection Heading Here}
% Subsubsection text here.

\section{Dataset}
The Dataset consists in a collection of taxi trip records in some city, including fields for capturing pick-up and drop-off dates/times, pick-up and drop-off locations and trip distances.

\section{PySpark Data Frames}
We used PySpark Data Frames to create indexes for answering the following queries:
\subsection{How many trips were started in each year present in the data set?}
With a simple script, built in fluent mode, we applied an aggregation function \textit{count} on top of an aggregation by the year of the trip.
\par
Table \ref{output_1_1} shows an example of the obtained data. \par
The obtained performance counters were: user 5.33 ms, sys: 3.94 ms, total: 9.27 ms

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Results of 1.1}
\label{output_1_1}
\centering
\begin{tabular}{c||c}
\hline
\bfseries year & \bfseries \#trips\\
\hline\hline
2013 & 54409\\
2014 & 74753\\
... & ...\\
2019 & 32797\\
2020 & 6829 \\
\hline
\end{tabular}
\end{table}

\subsection{For each of the 24 hours of the day, how many taxi trips there were, what was their average trip miles and trip total cost?}
This problem is solved by a simple grouping by the trip hour. In this case, instead of computing a single kpi we obtained the value of 3 aggregation functions over the same aggregation.

Table \ref{output_1_2} shows an example of the obtained data. \par
The obtained performance counters were: user 16.5 ms, sys: 8.98 ms, total: 25.5 ms.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Results of 1.2}
\label{output_1_2}
\centering
\begin{tabular}{c||r|c|c}
\hline

\bfseries time\textunderscore slot & \bfseries \#trips & \bfseries avg\textunderscore trip\textunderscore miles & \bfseries avg\textunderscore trip\textunderscore total\\
\hline\hline
05 PM    & 23637 & 3.04          & 15.16  \\
08 PM    & 22218 & 3.18          & 15.41  \\
11 AM    & 18616 & 3.42          & 15.41  \\
10 AM    & 17775 & 3.20          & 15.22  \\
04 AM    & 4604  & 3.80          & 15.19  \\
...    & ... & ... & ... \\
11 PM    & 16299 & 2.86          & 14.32  \\
06 AM    & 5628  & 5.25          & 19.92  \\
09 PM    & 19783 & 3.31          & 15.44  \\
10 PM    & 18491 & 3.11          & 14.95  \\
09 AM    & 18246 & 3.10          & 14.47  \\
03 PM    & 20704 & 3.50          & 16.26  \\

\hline
\end{tabular}
\end{table}


\subsection{For each of the 24 hours of the day, which are the 5 most popular routes according to the the total number of taxi trips? Also report and the average fare.}
This problem requires the computation of a ranking over one key. We used Spark's Data Frames windowing functions   provided by the Window class.
Table \ref{output_1_3} shows an example of the obtained data. \par
The obtained performance counters were: user 35.7 ms, sys: 14.5 ms, total: 50.2 ms

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Results of 1.3}
\label{output_1_3}
\centering
\begin{tabular}{c||c|c|c}
\hline
\bfseries pickup\_location & \bfseries \#trips & \bfseries a\_miles & \bfseries a\_total \\
\hline\hline
05 PM 17031839100 17031839100 & 466 & 0.58 & 6.66 \\
05 PM 17031839100 17031281900 & 327 & 0.58 & 6.56 \\
05 PM 17031839100 17031320100 & 292 & 0.69 & 7.20 \\
... & ... & ... & ... \\
05 PM 17031839100 17031280100 & 199 & 0.67 & 6.77 \\
08 PM 17031839100 17031839100 & 183 & 0.49 & 6.17 \\

\hline
\end{tabular}
\end{table}

\subsection{EXTRA: What's the impact of the fuel price in the ride total?}
We stared by obtaining the historic prices of gas in the the U. S. \cite{IEEEhowto:oil_price}.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Historic average prices of gas in the U. S. \cite{IEEEhowto:oil_price} - Matrix format}
\label{oil_prices}
\centering
\begin{tabular}{c||c|c|c|c|c|c|c}
\hline
\bfseries Year&  \bfseries Jan&  \bfseries Feb& \bfseries Mar& \bfseries Apr& \bfseries May& \bfseries Jun& \bfseries ...\\
\hline\hline
2006& 2.36&2.326&2.468&2.787&2.953& 2.93&...\\
2007&2.289&2.323&2.609&2.891&3.187&3.102&...\\
2008&3.095&3.078&3.293&3.507&3.815&4.105&...\\
2009& 1.84&1.975&2.011&2.102&2.316&2.681&...\\
2010&2.769&2.699&2.824&  2.9& 2.89&2.785&...\\
2011&3.148&3.264&3.615&3.852& 3.96&3.735&...\\
2012& 3.44& 3.64&3.907&3.958&3.791&3.596&...\\
...&...&...&...&...&...&...&...\\
\hline
\end{tabular}
\end{table}

Unfortunately the data wasn't in tabular format but in a matrix of years x months, like presented on Table \ref{oil_prices}. Spark Data Frames has a very handy expression \textit{stack} for unpivoting matrices, transforming it into tables. After unpivoting the data we organised the data like shown in Table \ref{oil_prices_tabular}.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Historic average prices of gas in the U. S.\cite{IEEEhowto:oil_price} - Tabular Format }
\label{oil_prices_tabular}
\centering
\begin{tabular}{c|c||c}
\hline
\bfseries Year& \bfseries Month& \bfseries Price\\
\hline\hline
1994&  Jan&0.998\\
1994&  Feb&1.009\\
1994&  Mar&1.008\\
1995&  Jan& 1.13\\
1995&  Feb& 1.12\\
1995&  Mar&1.119\\
...&  ...&...\\
\hline
\end{tabular}
\end{table}

We joined this data with the taxi rides data and we could calculate the cost of the consumed gas during each trip, taking into account an average consumption of 26.3 mpg \cite{IEEEhowto:fuel_eco}. Table \ref{output_1_4} shows an example of the obtained data. \par
We can conclude that although the gas price oscillated substantially it always represented slightly the same proportion in the total bill, around 2.2\%.
\par\par
The obtained performance counters were: user 29.4 ms, sys: 20.1 ms, total: 49.5 ms

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Results of 1.4}
\label{output_1_4}
\centering
\begin{tabular}{c||c|c|c}
\hline
\bfseries year & \bfseries \% gas cost on total& \bfseries avg gas price \$/g & \bfseries \$/mile\\
\hline\hline
2013&2.2                &3.57             &6.17  \\
2014&2.68               &3.46             &4.90   \\
2015&2.06               &2.53             &4.68  \\
2016&2.02               &2.25             &4.26  \\
2017&2.19               &2.53             &4.38  \\
2018&2.37               &2.83             &4.54  \\
2019&2.18               &2.69             &4.72  \\
2020&1.98               &2.47             &4.70   \\
\hline
\end{tabular}
\end{table}

\section{Spark SQL and Hive}
We used Spark SQL and Hive to create indexes for answering the queries presented in the subsections bellow. The implementation is almost the same for both technologies as all the business logic is implemented in the SQL query.

For hive we stared by creating a table, accordingly with the supplied csv file structure. For the columns containing date/time values we needed to instruct the serde about the regular expression of the date and time representation. For Spark SQL we also needed to indicate the regular expression of the Timestap columns, here in the read method for dataframe creation.

\subsection{What is the accumulated number of taxi trips per month?}
We developed a small SQL query for grouping the data per month and count the number or trips. The query run, without any modification, on both Hive and Spark SQL. Table \ref{output_2_1} shows an example of the obtained data.
\par

The obtained performance counters can be checked on Table \ref{perf_2_1}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Performance of 2.1}
\label{perf_2_1}
\centering
\begin{tabular}{c|c}
\hline
\bfseries Hive & \bfseries Spark SQL \\
\hline\hline
14.62 s          & 44.1 ms    \\
\hline
\end{tabular}
\end{table}


\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Results of 2.1}
\label{output_2_1}
\centering
\begin{tabular}{c||c}
\hline
\bfseries month\_number & \bfseries \#total\_trips\\
\hline\hline
12           & 29252       \\
01           & 30357       \\
...           & ...       \\
03           & 35260       \\
05           & 34979       \\
\hline
\end{tabular}
\end{table}



\subsection{For each pickup region, report the list of unique dropoff regions?}

Here we used the \textit{collect\_list} aggregation function to concatenate all the aggregated lines in a single list. We also use a sub query for eliminate duplicates using the \textit{distinct} key word.

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Results of 2.2}
\label{output_2_2}
\centering
\begin{tabular}{c||c}
\hline
\bfseries pickup\_region\_ID & \bfseries list\_of\_dropoff\_region\_ID\\
\hline\hline

17031770202 &['17031770202']\\
17031020301 &['17031020802', '17031020301']\\
17031837100 &['17031243000', '17031320400', '17031837100']\\
17031030104 &['17031030104', '17031980000', '17031280100']\\
17031040300 &['17031320400', '17031040300']\\
... &...\\
\hline
\end{tabular}
\end{table}

The obtained performance counters can be checked on Table \ref{perf_2_2}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Performance of 2.2}
\label{perf_2_2}
\centering
\begin{tabular}{c|c}
\hline
\bfseries Hive & \bfseries Spark SQL \\
\hline\hline
15.50 s         & 190 ms    \\
\hline
\end{tabular}
\end{table}


\subsection{What is the expected duration and distance of a taxi ride, given the pickup region ID, the weekday (0=Monday, 6=Sunday) and time in format “hour AM/PM”?}

For presenting the weekday we haven't found a common approach for Spark SQL and for Hive. For Spark SQL we developed an UDF as the Spark \textit{date\_format} hasn't a numeric representation for the weekday. Hive has the capability to extract the numeric representation of the weekday, however it isn't aligned with the specification, so we needed to convert the weekday to integer and subtract 1.

 Table \ref{output_2_3} shows an example of the obtained data. \par


\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Results of 2.3}
\label{output_2_3}
\centering
\begin{tabular}{c||c}
\hline
\bfseries pickup              & \bfseries avg\_total\_trip\_cost\\
\hline\hline
17031081600\_3\_12\_PM & 5.85               \\
17031071500\_5\_12\_PM & 3.25               \\
... & ...   \\
17031081500\_5\_12\_AM & 12.45              \\
17031842300\_1\_08\_AM & 43.8               \\
17031081000\_5\_03\_PM & 7.05               \\
\hline
\end{tabular}
\end{table}

The obtained performance counters can be checked on Table \ref{perf_2_3}.
The UDF highly penalised the performance. We could try other approaches like a case condition for each textual weekday or a join with a temporary table with pairs (Mon, 0),(Tue, 1)...
\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Performance of 2.3}
\label{perf_2_3}
\centering
\begin{tabular}{c||c|c}
\hline
\bfseries Mode &\bfseries Hive & \bfseries Spark SQL \\
\hline\hline
With Weekday (UDF for Spark) & 16.77 s         & 3.85 s    \\
No Weekday & 15.33 s  & 4.87 ms \\
\hline
\end{tabular}
\end{table}


\subsection{EXTRA: Which are the top 3 companies driving from the most popular locations? (more than 5000 trips from there)}
We observe that the \textit{Taxi Affiliation Services} company dominates the trips from the most popular locations in the city followed by \textit{Flash Cab}, the third place is shared between several smaller providers. \par
As this insight is complex we needed to create 4 sub-queries using the SQL \textbf{with} statement. For being able to obtaining the 3 ranking measures, top 3 and bugger than 5000 trips, we needed to use \textbf{windowing functions}. \par
Even with complex queries like this one the syntax on Hive and on Spark SQL was exactly the same.


\begin{table}[!t]
\renewcommand{\arraystretch}{1.4}
\caption{Results of  1.4}
\label{perf_1_4}
\centering
\begin{tabular}{c||l|c}
\hline
\bfseries pickupregionid &\bfseries company   & \bfseries ntrips \\
\hline\hline
17031839100 &Taxi Affiliation Services        & 8485  \\
17031839100 &Flash Cab                        & 3485  \\
17031839100 &Yellow Cab                       & 2308  \\
\hline
17031320100 &Taxi Affiliation Services        & 4567  \\
17031320100 &Flash Cab                        & 1768  \\
17031320100 &Blue Ribbon Taxi Association Inc.& 1396  \\
\hline
17031980000 &Taxi Affiliation Services        & 2955  \\
17031980000 &Flash Cab                        & 1404  \\
17031980000 &Dispatch Taxi Affiliation        & 950   \\
\hline
17031081500 &Taxi Affiliation Services        & 3452  \\
17031081500 &Flash Cab                        & 1201  \\
17031081500 &Choice Taxi Association          & 1002  \\
\hline
17031081700 &Taxi Affiliation Services        & 3110  \\
17031081700 &Flash Cab                        & 1476  \\
17031081700 &Dispatch Taxi Affiliation        & 997   \\
\hline
17031281900 &Taxi Affiliation Services        & 2736  \\
17031281900 &Chicago Carriage Cab Corp        & 1063  \\
17031281900 &Flash Cab                        & 891   \\
\hline
17031081403 &Taxi Affiliation Services        & 2313  \\
17031081403 &Flash Cab                        & 814   \\
\hline
\end{tabular}
\end{table}

The obtained performance counters can be checked on Table \ref{perf_2_4}

\begin{table}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Performance of 2.4}
\label{perf_2_4}
\centering
\begin{tabular}{c|c}
\hline
\bfseries Hive & \bfseries Spark SQL \\
\hline\hline
12.67 s        & 6.13 ms    \\
\hline
\end{tabular}
\end{table}


\section{Conclusion}


\begin{itemize}
\item HIVE language is designated HQL, its syntax isn't filly SQL compliant however in our case we haven't found any particular limitation.
\item In Big Data, like in most of the computing frameworks, working with dates and times is often tricky. Most of the particularities were solved at the beginning during the Table creation for Hive and during the first dataframe instantiation for both Spark technologies.

\end{itemize}

\subsection{Performance}
\begin{itemize}
\item We collected the ELAPSED TIME measure of Hive and the total CPU time for Spark. This means we don't consider the time for rendering results, as we have different outputs CLI and Jupyter Notebook.
\item For being able to measure the computation of the full DAG of our Spark exercises we executed a \textit{cache()} action. Otherwise the \textit{show()} function would only compute the necessary to show an output sample. This was valid for Spark Data frames as well as for Spark SQL.
\item UDF functions in Spark SQL highly penalise performance.
\item Hive performance is much poor, in most cases, when compared with Spark SQL. However this isn't always true as today Hive has engines, other than MapReduce or Tez, like LLAP that promise very good results for interactive querying.
\end{itemize}




% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{IEEEhowto:bdpsp1}
J. Lourenço, \emph{Big Data Processing Systems — Project nº 2}, 2020/21.\hskip 1em plus
  0.5em minus 0.4em\relax DI, FCT, UNL, PT.

\bibitem{IEEEhowto:bdps_lect}
J. Lourenço, \emph{Big Data Processing Systems — Class Lectures}, 2020/21.\hskip 1em plus
  0.5em minus 0.4em\relax DI, FCT, UNL, PT.
\bibitem{IEEEhowto:test}
Thomas Erl, Wajid Khattak and Paul Buhler \emph{Big Data Fundamentals}, 2016\hskip 1em plus 0.5em minus 0.4em\relax Prentice Hall.
\bibitem{IEEEhowto:oil_price} U.S. Energy Information Administration \emph{U.S. All Grades All Formulations Retail Gasoline Prices (Dollars per Gallon)}
\bibitem{IEEEhowto:fuel_eco} Wikipedia \emph{Fuel economy in automobiles}

\end{thebibliography}

% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section*{Work division}
\else
  % regular IEEE prefers the singular form
  \section*{Work division}
\fi


59290 - Helder Rodrigues - All



% that's all folks
\end{document}


